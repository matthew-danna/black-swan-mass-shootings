# clear the workspace
#rm(list=ls())

##### VIZ
# http://www.thinkingondata.com/portfolio/nobel-prize-awards-by-gender-using-chord-diagram/
# https://michaeltoth.me/you-need-to-start-branding-your-graphs-heres-how-with-ggplot.html
# https://cran.r-project.org/web/packages/ggbeeswarm/vignettes/usageExamples.pdf
# https://blog.rsquaredacademy.com/introducting-vistributions/
# https://plot.ly/ggplot2/stat_smooth/
# http://gradientdescending.com/adding-custom-fonts-to-ggplot-in-r/
# http://www.deeplytrivial.com/2019/03/statistics-sunday-scatterplots-and.html
# https://ropensci.org/blog/2019/04/02/hydrology-task-view/
# https://geekcologist.wordpress.com/2018/09/21/multiplot-with-ggplot/
# https://taraskaduk.com/2019/03/23/apple-health/
# https://luisdva.github.io/rstats/model-cluster-plots/
# https://www.stevejburr.com/post/scatter-plots-and-best-fit-lines/
# https://dominicroye.github.io/en/2019/calculating-the-distance-to-the-sea-in-r/

##### STATS
# Logistic regression in R using blorr package
# https://blog.rsquaredacademy.com/introducing-blorr/
# Some R Packages for ROC Curves
# https://rviews.rstudio.com/2019/03/01/some-r-packages-for-roc-curves/
# https://datascienceplus.com/bayesian-statistics-analysis-of-health-data/
# https://toscano84.github.io/2019/03/clustering-the-pharmaceutical-industry-stocks/
# https://mattkmiecik.com/post-Exploring-11-Years-of-Chicago-Blackhawks-Data-using-Principal-Components-Analysis.html
# https://dominicroye.github.io/en/2019/tidy-correlation-tests-in-r/
# https://statisticaloddsandends.wordpress.com/2019/04/08/many-ways-to-do-the-same-thing-linear-regression/

#### DATA
# https://github.com/hrecht/censusapi
# https://github.com/walkerke/tidycensus

# THE DELTA METHOD AND ITS IMPLEMENTATION IN R
# https://sciprincess.wordpress.com/2019/03/01/the-delta-method-and-its-implementation-in-r/

# Robust Regressions: Dealing with Outliers in R
# https://datascienceplus.com/robust-regressions-dealing-with-outliers-in-r/



# Plots within plots with ggplot2 and ggmap
# https://statisticaloddsandends.wordpress.com/2019/02/24/plots-within-plots-with-ggplot2-and-ggmap/

# BBC style graphics
# https://bbc.github.io/rcookbook/

# determining clusters
# https://towardsdatascience.com/10-tips-for-choosing-the-optimal-number-of-clusters-277e93d72d92

# Navigate through Decennial Census and American Community Survey
# https://www.gl-li.com/2019/02/03/navigate-through-decennial-census-and-american-community-survey/

# Multiple Data (Time Series) Streams Clustering
# https://petolau.github.io/Multiple-data-streams-clustering-in-r/

# for correlation graphing!
# http://blog.ephorie.de/learning-data-science-modelling-basics

# Tables gallery
# https://finalfit.org/articles/tables_gallery.html

# for creating hotspot/clusters for BS shootings
# https://datascienceplus.com/visualizing-new-york-city-wifi-access-with-k-means-clustering/

# Fast Static Maps Built with R
# https://rud.is/b/2019/01/30/fast-static-maps-built-with-r/

# Spatial lag model trees
# https://eeecon.uibk.ac.at/~zeileis/news/lagsarlmtree/

# How to Make Animated (GIF) Heatmaps in R
# https://flowingdata.com/2019/01/15/how-to-make-animated-gif-heatmaps-in-r/

# Factor Analysis in R with Psych Package: Measuring Consumer Involvement
# https://lucidmanager.org/measuring-consumer-involvement/

# colors
# https://eeecon.uibk.ac.at/~zeileis/news/colorspace/

# ggeffects 0.8.0 now on CRAN: marginal effects for regression models #rstats
# https://strengejacke.wordpress.com/2019/01/14/ggeffects-0-8-0-now-on-cran-marginal-effects-for-regression-models-rstats/

# Correlation analysis of cyclically adjusted valuation measures and subsequent returns
# http://databasedinvesting.blogspot.com/2019/01/correlation-analysis-of-cyclically.html

# team color codes
# https://teamcolorcodes.com/nba-team-color-codes/

# EDA
# https://blog.datascienceheroes.com/exploratory-data-analysis-data-preparation-with-funmodeling/

# 10 years of playback history on Last.FM: "Just sit back and listen"
# color coded graph by year
# http://rcrastinate.blogspot.com/2019/01/10-years-of-playback-history-on-lastfm.html

# multiple ggplots
# https://datascienceplus.com/how-to-combine-multiple-ggplot-plots-to-make-publication-ready-plots/
# http://www.seascapemodels.org/data/data-wrangling-spatial-course.html#introduction

# Census API info
# https://www.census.gov/data/developers/updates/api-format-changes--2010-2014-acs-estimates.html
# https://www.census.gov/data/developers/updates/api-format-changes--sf1-2010.html

# Census
# https://mattherman.info/blog/tidycensus-mult/
# https://mattherman.info/blog/tidycensus-mult-year/
# https://blog.revolutionanalytics.com/2018/11/working-with-us-census-data-in-r.html

#  Principle Component Analysis and K-Means Clustering
# https://annamarbut.blogspot.com/2018/12/submitter-segmentation.html

# Correlation matrix
# https://www.displayr.com/how-to-create-a-correlation-matrix-in-r/

# GETTING CENSUS DATA FROM MULTIPLE STATES USING TIDYCENSUS AND PURRR
# https://mattherman.info/blog/tidycensus-mult/

# Determining Optimal Number Of Clusters In Your Data
# https://kkulma.github.io/2017-04-24-determining-optimal-number-of-clusters-in-your-data/

# POINT-IN-POLYGON WITH SF
# https://mattherman.info/blog/point-in-poly/

# styling scatter plots cleanly
# https://www.jumpingrivers.com/blog/styling-base-r-graphics/

# outliers
# http://blog.revolutionanalytics.com/2018/03/outliers.html

# reading in google sheet as a csv
# us_mass_shootings <- 
#  readr::read_csv("https://docs.google.com/spreadsheet/pub?key=0AswaDV9q95oZdG5fVGJTS25GQXhSTDFpZXE0RHhUdkE&output=csv")

# scatterplots
# https://www.r-graph-gallery.com/scatterplot/

# ANOMALY DETECTION
# https://www.business-science.io/code-tools/2018/04/08/introducing-anomalize.html
# https://analyzecore.com/2018/06/13/anomaly-detection-for-business-metrics-with-r/
# https://cran.r-project.org/web/packages/anomaly/index.html
# https://blog.twitter.com/engineering/en_us/a/2015/introducing-practical-and-robust-anomaly-detection-in-a-time-series.html
# https://blog.datascienceheroes.com/anomaly-detection-in-r/
# https://anomaly.io/anomaly-detection-twitter-r/
# https://blog.exploratory.io/introduction-to-anomaly-detection-in-r-with-exploratory-a0507d40385d
# https://cran.r-project.org/web/packages/anomalyDetection/index.html
# https://github.com/twitter/AnomalyDetection/blob/master/R/detect_anoms.R
# http://blog.revolutionanalytics.com/2015/01/twitters-new-r-package-for-anomaly-detection.html

# Getting started with Negative Binomial Regression Modeling
# https://data.library.virginia.edu/getting-started-with-negative-binomial-regression-modeling/

# HOW TO BUILD A LOGISTIC REGRESSION MODEL FROM SCRATCH IN R
# http://theautomatic.net/2018/10/02/how-to-build-a-logistic-regression-model-from-scratch-in-r/

# ggplot extensions
# https://www.ggplot2-exts.org/gallery/

# effect sizes
# https://www.rdocumentation.org/packages/effsize/versions/0.7.1/topics/cohen.d
if (!require(effsize)) install.packages('effsize')
library(effsize)

#### 0: load packages
if (!require(devtools)) install.packages('devtools')
devtools::install_github("brooke-watson/BRRR")
library(BRRR)
if (!require(googlesheets)) install.packages('googlesheets')
library(googlesheets)
## check and verify still needed
if (!require(tidyverse)) install.packages('tidyverse')
library(tidyverse)
if (!require(ggthemes)) install.packages('ggthemes')
library(ggthemes)
if (!require(portfolio)) install.packages('portfolio')
library(portfolio)
if (!require(moments)) install.packages('moments')
library(moments)
if (!require(ggplot2)) install.packages('ggplot2')
library(ggplot2)
devtools::install_github("edwinth/paletti")
library(paletti)
devtools::install_github("kassambara/ggpubr")
library(ggpubr)
if (!require(quantmod)) install.packages('quantmod')
library(quantmod)
if (!require(stats)) install.packages('stats')
library(stats)
if (!require(stringr)) install.packages('stringr')
library(stringr)
if (!require(dplyr)) install.packages('dplyr')
library(dplyr)
devtools::install_github("twitter/AnomalyDetection")
library(AnomalyDetection)
devtools::install_github("johannesbjork/LaCroixColoR")
library(LaCroixColoR)
if (!require(data.table)) install.packages('data.table')
library(data.table)
if (!require(formattable)) install.packages('formattable')
library(formattable)
if (!require(tidyr)) install.packages('tidyr')
library(tidyr)
if (!require(extrafont)) install.packages('extrafont')
library(extrafont)
if (!require(magick)) install.packages('magick')
library(magick)
skrrrahh(0)

#### 01: load graph images and themess
logo <- image_read("https://avatars3.githubusercontent.com/u/46403937?s=400&u=8cfe7dc584e41591b4cebd50d03e62d0729a3bcd&v=4")

theme_me <- function(base_size = 14, base_family = "Liberation Serif") {
  library(grid)
  (theme_foundation(base_size = base_size, base_family = base_family)
    + theme(plot.title = element_text(face = "bold",
                                      size = rel(1.2),
                                      margin = margin(b = 0)),
            text = element_text(),
            panel.background = element_rect(colour = NA),
            plot.background = element_rect(colour = NA),
            plot.subtitle = element_text(margin = margin(t = 5, b = 10)),
            panel.border = element_rect(colour = NA),
            axis.title = element_text(face = "bold", size = rel(1)),
            axis.title.y = element_blank(),
            axis.title.x = element_blank(),
            axis.text.y = element_blank(),
            axis.text.x = element_text(size = 9, angle = 90, vjust = 0.2,
                                       margin = margin(-5, unit = "mm")), 
            axis.line = element_blank(),
            axis.ticks = element_blank(),
            panel.grid.major = element_blank(),
            panel.grid.minor = element_blank(),
            legend.key = element_rect(colour = NA),
            legend.position = "bottom",
            legend.direction = "horizontal",
            legend.key.size = unit(0.2, "cm"),
            legend.title = element_text(face = "italic"),
            legend.text = element_text(size = rel(1)),
            plot.margin = unit(c(10,5,5,5), "mm"),
            strip.background = element_rect(colour = "#f0f0f0", fill = "#f0f0f0"),
            strip.text = element_text(face = "bold")
    ))
}

customGreen0 = "#DeF7E9"
customGreen = "#71CA97"
customRed = "#ff7f7f"

skrrrahh(0)

#### 02: load files
# log in, then resume
gs_auth(new_user = TRUE)

data.sources <- gs_title("data sources")
data.sources.df <- gs_read(data.sources, ws = "blackswan datasources")

shooting.sample <- gs_title("data subset")
shooting.sample.df <- gs_read(shooting.sample)

bs.data <- gs_title("all mass shootings 1949-2018")
bs.data.df <- gs_read(bs.data)

skrrrahh(0)


#### 03: analysis

#### 04: graphs

#### 05: outputs

# data sources: treemap, super clean table, and graph
data.sources <- gs_title("data sources")
data.sources.df <- gs_read(data.sources, ws = "blackswan datasources")
colnames(data.sources.df) <- as.character(unlist(data.sources.df[1,]))
data.sources.df <- data.sources.df[-1, ]
data.sources.df <- data.sources.df[1:18,]
data.sources.df$ID <- seq.int(nrow(data.sources.df))
names(data.sources.df)[names(data.sources.df) == 'U.S. Shootings'] <- 'US.Shootings'
data.sources.df$US.Shootings <- gsub(",","",data.sources.df$US.Shootings)
data.sources.df$US.Shootings <- as.numeric(data.sources.df$US.Shootings)

map.market(id = data.sources.df$Short.Title, area = data.sources.df$US.Shootings, 
           color = (data.sources.df$US.Shootings),
           group = data.sources.df$Short.Title, main = "U.S. Mass Shooting Datasets")

data.sources.table <- data.sources.df[c(2,4:10)]
names(data.sources.table)[names(data.sources.table) == 'US.Shootings'] <- 'Mass Shootings'

formattable(data.sources.table, 
            align =c("l","c","c","c","c","c","c","c","c"), 
            list(`Type` = formatter(
              "span", style = ~ style(color = "grey",font.weight = "bold")),
              `Mass Shootings`= color_tile(customGreen, customRed),
              `Victim Count Threshold`= color_tile(customGreen, customRed),
              `Temporal Start`= color_tile(customRed, customGreen),
              `Temporal End`= color_tile(customRed, customGreen)
            ))

# figure 03
title.data.sources <- expression(paste(bold("Black Swan Shootings: "), 
                                       "Data sources by event counts"))

plot.data.sources <- ggplot(data = data.sources.df, aes(x = reorder(Short.Title, -US.Shootings), 
                                                             y = US.Shootings)) +
  geom_bar(stat = "identity", width = .75, fill = "gray", color = "black") +
  labs(title = title.data.sources,
       x = "Data Source",
       y = "Mass Shootings Count",
       caption = expression(bold("Figure 03"))) +
  geom_text(aes(label=US.Shootings), vjust = 1, color = "black", size = 3) +
  geom_vline(aes(xintercept = 3.5), color = "red", linetype = "dashed", size = 1.5) +
  geom_vline(aes(xintercept = 8.5), color = "red", linetype = "dashed", size = 1.5) +
  annotate("rect", xmin = -Inf, xmax = 3.5, ymin = 0, ymax = Inf, alpha = 0.3) +
  annotate("rect", xmin = 3.5, xmax = 8.5, ymin = 0, ymax = Inf, alpha = 0.15) +
  theme_me()

plot.data.sources
grid::grid.raster(logo, x =.01, y = .005, just = c('left','bottom'), width = unit(0.55,'inches'))
skrrrahh(0)

# get subset data, graph
shooting.sample <- gs_title("data subset")
shooting.sample.df <- gs_read(shooting.sample)
shooting.sample.df$kill.wound <- round(shooting.sample.df$Killed/shooting.sample.df$Wounded, 
                                       digits = 2)

plot.kill.wound <- ggplot(shooting.sample.df, aes(x=Killed, y=Wounded)) +
  geom_point(color = "red")
plot.kill.wound + ggtitle("MST and GVA Mass Shootings, 2013-2018") + 
  theme(plot.title = element_text(hjust = 0.5))
skrrrahh(0)

# sample data central tendency summary stats
Type <- c("Killed", "Wounded", "Total Casualties")
Mean <- round(c(mean(shooting.sample.df$Killed), mean(shooting.sample.df$Wounded), 
         mean(shooting.sample.df$Total)), digits = 2)
Median <- c(median(shooting.sample.df$Killed), median(shooting.sample.df$Wounded),
           median(shooting.sample.df$Total))
getmode <- function(v) {
  uniqv <- unique(v)
  uniqv[which.max(tabulate(match(v, uniqv)))]
}
Mode <- c(getmode(shooting.sample.df$Killed), getmode(shooting.sample.df$Wounded), 
          getmode(shooting.sample.df$Total))
Skew <- round(c(skewness(shooting.sample.df$Killed), skewness(shooting.sample.df$Wounded),
         skewness(shooting.sample.df$Total)), digits = 2)
Standard.Deviation <- round(c(sd(shooting.sample.df$Killed), sd(shooting.sample.df$Wounded), 
         sd(shooting.sample.df$Total)), digits = 2)
Max <- c(max(shooting.sample.df$Killed), max(shooting.sample.df$Wounded), 
         max(shooting.sample.df$Total))
Min <- c(min(shooting.sample.df$Killed), min(shooting.sample.df$Wounded), 
         min(shooting.sample.df$Total))
shooting.sample.stats <- data.frame(Type, Mean, Median, Mode, Skew, Standard.Deviation, Max, Min)
rownames(shooting.sample.stats) <- NULL
skrrrahh(0)

# let's get pretty stats table
# for change arrows
change_formatter <- 
  formatter("span", 
            style = x ~ style(font.weight = "bold", 
                              color = ifelse(x > 0, customGreen, ifelse(x < 0, customRed, "black"))), 
            x ~ icontext(ifelse(x>0, "arrow-up", "arrow-down"), x)
  )

formattable(shooting.sample.stats, 
            align =c("l","c","c","c","c","c","c"), 
            list(`Type` = formatter(
              "span", style = ~ style(color = "grey",font.weight = "bold")),
              `Mean`= color_tile(customGreen, customGreen0),
              `Median`= color_tile(customGreen, customGreen0),
              `Mode`= color_tile(customGreen, customGreen0),
              `Skew`= color_tile(customGreen, customGreen0),
              `Standard.Deviation`= color_tile(customGreen, customGreen0),
              `Max`= color_tile(customGreen, customGreen0),
              `Min`= color_tile(customGreen, customGreen0),
              #`Average` = color_bar(customRed),
              `Change` = change_formatter
            ))
skrrrahh(0)

# data subset frequency stats
## total victims
total.freq <- table(shooting.sample.df$Total)
sample.total.freq <- as.data.frame(total.freq)
names(sample.total.freq)[1] = 'Victim.Counts'
sample.total.freq$Freq <- as.numeric(sample.total.freq$Freq)
sample.total.freq$pct.change <- (Delt(sample.total.freq$Freq))*100
sample.total.freq$cumsum <- cumsum(sample.total.freq$Freq)
sample.total.freq$pct <- (sample.total.freq$Freq/(sum(sample.total.freq$Freq)))*100
sample.total.freq$cumpct <- cumsum(sample.total.freq$pct)
skrrrahh(0)

## killed victims
killed.freq <- table(shooting.sample.df$Killed)
sample.killed.freq <- as.data.frame(killed.freq)
names(sample.killed.freq)[1] = 'Fatalities'
sample.killed.freq$Freq <- as.numeric(sample.killed.freq$Freq)
sample.killed.freq$pct.change <- (Delt(sample.killed.freq$Freq))*100
sample.killed.freq$cumsum <- cumsum(sample.killed.freq$Freq)
sample.killed.freq$pct <- (sample.killed.freq$Freq/(sum(sample.killed.freq$Freq)))*100
sample.killed.freq$cumpct <- cumsum(sample.killed.freq$pct)
skrrrahh(0)

# anomaly detections
## max_anoms = Max anomalies that S-H-ESD will detect as a percentage of the data
## alpha = level of statistical significance with which to accept or reject anomalies
# total
ad.total.twitter <- AnomalyDetectionVec(shooting.sample.df$Total, max_anoms = 0.01, 
                                 direction = "pos", alpha = 0.01, period = 2, 
                                 only_last = F, threshold = 'None', e_value = F, 
                                 longterm_period = NULL, plot = T, y_log = F, 
                                 xlabel = "Mass Shootings", ylabel = "Victim Count", 
                                 title = "Anomalies: Total Casualty Counts")
ad.total.twitter$plot
ad.total.twitter.anomaly.table=ad.total.twitter$anoms

#### find anom min, append as a value to mass shootings

# killed
ad.killed.twitter <- AnomalyDetectionVec(shooting.sample.df$Killed, max_anoms = 0.01, 
                                        direction = "pos", alpha = 0.01, period = 2, 
                                        only_last = F, threshold = 'None', e_value = F, 
                                        longterm_period = NULL, plot = T, y_log = F, 
                                        xlabel = "Mass Shootings", ylabel = "Victim Count", 
                                        title = "Anomalies: Killed Counts")
ad.killed.twitter$plot
ad.killed.twitter.anomaly.table=ad.killed.twitter$anoms

# wounded
ad.wounded.twitter <- AnomalyDetectionVec(shooting.sample.df$Wounded, max_anoms = 0.01, 
                                         direction = "pos", alpha = 0.01, period = 2, 
                                         only_last = F, threshold = 'None', e_value = F, 
                                         longterm_period = NULL, plot = T, y_log = F, 
                                         xlabel = "Mass Shootings", ylabel = "Victim Count", 
                                         title = "Anomalies: Wounded Counts")
ad.wounded.twitter$plot
ad.wounded.twitter.anomaly.table=ad.wounded.twitter$anoms

# dates
shooting.sample.df$dt <- as.Date(shooting.sample.df$Date, format='%m/%d/%Y')
shooting.sample.df$year <- str_sub(shooting.sample.df$NewDate, start = -4)

# temporal stuff
year_groups <- group_by(shooting.sample.df, year)
shooting.sample.year <- summarise(year_groups, n = n())

## better
plot.killed <- ggplot(shooting.sample.df, aes(Killed, fill = cut(Killed, 50))) +
  geom_histogram(binwidth = 1, show.legend = FALSE) +
  scale_fill_discrete(h = c(240, 10), c = 120, l = 70) +
  theme_minimal() +
  labs(x = "Fatality Count", y = "Event Count") +
  ggtitle("U.S. Mass Shootings: 2013-2018 (Mass Shooting Tracker and Gun Violence Archive)") + 
  theme(axis.text.x= element_text(face = "bold"), axis.text.y = element_text(face = "bold")) +
  scale_x_continuous(breaks=seq(0,(max(shooting.sample.df$Killed)+2),2)) +
  theme(axis.line = element_line(colour = "darkblue", 
                                  size = 1, linetype = "solid"))
plot.killed

# new histograms
# http://jethroemmanuel.netlify.com/2017/12/16/histogram-length-frequency-distribution-using-ggplot2/

# killed
range.killed <- max(shooting.sample.df$Killed) - min(shooting.sample.df$Killed)
class_size.killed <- round(range.killed/2)
class_interval.killed <- round(range.killed/class_size.killed)
lower_limit.killed <- seq(min(shooting.sample.df$Killed), max(shooting.sample.df$Killed), 
                          by = class_interval.killed)
upper_limit.killed <- (lower_limit.killed + class_interval.killed) - 0.1
bin.killed <- as.data.frame(cbind(lower_limit.killed, upper_limit.killed))
bin.killed <- bin.killed %>% 
  mutate(midlength.killed = (lower_limit.killed + upper_limit.killed) / 2)

killed.break.bjs <- 5
killed.break.research <- 4
killed.break.anomaly <- 8

title.sample <- expression(paste(bold("U.S. Mass Shootings: 2013-2018")))

plot.killed <- ggplot(data = shooting.sample.df, aes(x = Killed)) +
# geom_histogram(bins = class_size.killed, fill = "gray", color = "black") +
  geom_histogram(binwidth = 1, fill = "gray", color = "black") +
  scale_x_continuous(breaks = bin.killed$lower_limit.killed) +
  labs(title = title.sample,
       subtitle = "Proposed event cutoffs",
       x = "Fatality Count",
       y = "Event Count",
       caption = expression(paste(italic("Sources:"), "Mass Shooting Tracker, Gun Violence Archive"))) +
  geom_vline(aes(xintercept = killed.break.bjs), color = "green", linetype = "dashed", size = 0.8) +
  geom_vline(aes(xintercept = killed.break.research), color = "blue", linetype = "dashed", 
             size = 0.8) +
  geom_vline(aes(xintercept = killed.break.anomaly), color = "red", linetype = "dashed", size = 0.8) +
  geom_segment(aes(x = 15, y = 1000, xend = 4, yend = 1000),
               arrow = arrow(length = unit(.2, "cm"))) +
  geom_segment(aes(x = 15, y = 925, xend = 5, yend = 925),
               arrow = arrow(length = unit(.2, "cm"))) +
  geom_segment(aes(x = 15, y = 850, xend = 8, yend = 850),
               arrow = arrow(length = unit(.2, "cm"))) +
  annotate("text", x = 33, y = 1000, label = "Academic research breakpoint (256 potential events)") +
  annotate("text", x = 27.5, y = 925, label = "BJS breakpoint (81 potential events)") +
  annotate("text", x = 32, y = 850, label = "Anomaly detection breakpoint (18 potential events)") +
  annotate("rect", xmin = -Inf, xmax = 4, ymin = 0, ymax = Inf, alpha = 0.4) +
  annotate("rect", xmin = 4, xmax = 5, ymin = 0, ymax = Inf, alpha = 0.3) +
  annotate("rect", xmin = 5, xmax = 7.75, ymin = 0, ymax = Inf, alpha = 0.2) +
  annotate("rect", xmin = 8.25, xmax = 60, ymin = 0, ymax = 75, alpha = 0.05) +
  annotate("text", x = 35, y = 35, 
           label = expression(paste(bold("The 18 deadliest events since 2013")))) +
  theme_me()

plot.killed
grid::grid.raster(logo, x =.01, y = .005, just = c('left','bottom'), width = unit(0.55,'inches'))
skrrrahh(0)

###### fix 'anomaly detection breakpoint based on analyses

#ggarrange(h1, h2, h3, ncol = 1, nrow = 3)

# bs events
bs.events <- bs.data.df[c(1,6:14)]
bs.events <- subset(bs.events, BlackSwan == 'y')
bs.events$date <- as.Date(bs.events$date, format = '%m/%d/%Y')

bs.years <- group_by(bs.events, year)
counts.bs.year <- summarise(bs.years, n=n())
counts.bs.year$n <- as.numeric(counts.bs.year$n)
counts.bs.year$xlabel <- counts.bs.year$year
counts.bs.year$ylabel <- counts.bs.year$n
year <- c("1950","1951","1952","1953","1954","1955","1956","1957","1958","1959","1960","1961",
             "1962","1963","1964","1965","1967","1968","1969","1970","1971","1972","1974","1975",
             "1976","1977","1978","1979","1980","1985","1988","1992","1995","1996","1997","2000",
             "2002","2004")
missing <- as.data.frame(year)
missing$n <- "0"
missing$n <- as.numeric(missing$n)
missing$xlabel <- ""
missing$ylabel <- ""
counts.bs.year <- rbind(counts.bs.year, missing)
bs.year <- counts.bs.year[order(counts.bs.year$year),]

# figure 10: attacks per year
title.bs.years <- expression(paste(bold("Black Swan Shootings: "),"Attacks per year"))

plot.bs.years <- ggplot(data = bs.year, aes(x = bs.year$year,y = bs.year$n)) +
  geom_bar(stat = "identity", width = 1, fill = "red", color = "gray") +
  labs(title = title.bs.years,
       caption = expression(bold("Figure 10"))) +
  scale_x_discrete(labels = bs.year$xlabel) +
  geom_text(aes(label=ylabel), vjust = 1, color = "black", size = 3) +
  theme_me()

plot.bs.years
grid::grid.raster(logo, x =.01, y = .005, just = c('left','bottom'), width = unit(0.55,'inches'))
skrrrahh(0)

# intervals
bs.events <- bs.events[order(bs.events$date),]
bs.events$newID <- seq.int(nrow(bs.events))
bs.events.sub <- subset(bs.events, bs.events$newID > '1')
bs.events.sub$newID <- seq.int(nrow(bs.events.sub))
names(bs.events.sub)[names(bs.events.sub) == 'date'] <- 'date2'
bs.events.sub <- bs.events.sub[c(3,11)]
bs.events <- bs.events %>% left_join(bs.events.sub, by = 'newID')
bs.events$interval

#################################################
#################################################
# ecdf cumulative density
ecdf.killed = ecdf(sample.killed.freq$freq)
plot(ecdf.killed, xlab = "Number of Victims", ylab = "Cum. PCT of Events", 
     main = "Killed")
ecdf.injured = ecdf(sample.injured.freq$freq)
plot(ecdf.injured, xlab = "Number of Victims", ylab = "Cum. PCT of Events", 
     main = "Injured")
ecdf.total = ecdf(sample.total.freq$freq)
plot(ecdf.total, xlab = "Number of Victims", ylab = "Cum. PCT of Events", 
     main = "Killed + Injured")

# anomaly detection 2: z scores
shooting.sample.df$id.ch <- as.character(shooting.sample.df$ID)
shooting.sample.ad02 <- shooting.sample.df[c(-1)]
## formulas
# non-outlier z score
isnt_out_z <- function(x, thres = 3, na.rm = TRUE) {
  abs(x - mean(x, na.rm = na.rm)) <= thres * sd(x, na.rm = na.rm)
}
# z score with median absolute deviation
isnt_out_mad <- function(x, thres = 3, na.rm = TRUE) {
  abs(x - median(x, na.rm = na.rm)) <= thres * mad(x, na.rm = na.rm)
}
# tukey fences
isnt_out_tukey <- function(x, k = 1.5, na.rm = TRUE) {
  quar <- quantile(x, probs = c(0.25, 0.75), na.rm = na.rm)
  iqr <- diff(quar)
  (quar[1] - k * iqr <= x) & (x <= quar[2] + k * iqr)
}
# definition of non-outlier row
isnt_out_funs <- funs(
  z = isnt_out_z,
  mad = isnt_out_mad,
  tukey = isnt_out_tukey
)
## run
ad.02.columns <- shooting.sample.ad02 %>% transmute_if(is.numeric, isnt_out_funs)
ad.02 = data.frame(shooting.sample.ad02$id.ch, shooting.sample.ad02$Total,
                   ad.02.columns$Total_z, ad.02.columns$Total_tukey)
ad.02.falses <- subset(ad.02, ad.02.columns.Total_z=="FALSE" | 
                         ad.02.columns.Total_tukey=="FALSE")

####

# some data
stanford <- read.csv("https://raw.githubusercontent.com/StanfordGeospatialCenter/MSA/master/Data/Stanford_MSA_Database.csv", 
                     stringsAsFactors = FALSE)